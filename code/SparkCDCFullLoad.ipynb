{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Data Capture\n",
    "### with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/04 22:39:30 WARN Utils: Your hostname, DESKTOP-SLEQT56 resolves to a loopback address: 127.0.1.1; using 172.25.13.138 instead (on interface eth0)\n",
      "24/04/04 22:39:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/04 22:39:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CDC\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cargamos el archivo __FULL LOAD__ y le cambiamos el nombre a las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-------------+\n",
      "| id|        fullName|         City|\n",
      "+---+----------------+-------------+\n",
      "|  0|Herman Zimmerman|Oklahoma City|\n",
      "|  1|        Lisa Ray|     Columbus|\n",
      "|  2|  Terrell Reeves| Jacksonville|\n",
      "|  3|   Steve Goodwin|    Charlotte|\n",
      "|  4|       Leah Tran|      Detroit|\n",
      "+---+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fldf = spark.read.csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/CDC_data/LOAD00000001.csv\")\n",
    "fldf = fldf.withColumnRenamed(\"_c0\", \"id\").\\\n",
    "    withColumnRenamed(\"_c1\", \"fullName\").\\\n",
    "        withColumnRenamed(\"_c2\", \"City\")\n",
    "fldf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creamos el archivo el un nuevo directorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fldf.write.mode(\"overwrite\").\\\n",
    "    csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/CDC_data/final_output/finalFile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cargamos el archivo __Replication onGoing__ que contiene los cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf = spark.read.csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/CDC_data/20240405-003052555.csv\")\n",
    "udf = udf.withColumnRenamed(\"_c0\", \"action\").\\\n",
    "    withColumnRenamed(\"_c1\", \"id\").\\\n",
    "        withColumnRenamed(\"_c2\", \"fullName\").\\\n",
    "            withColumnRenamed(\"_c3\", \"city\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Cargamos el archivo __fullLoad__ que copiamos en el paso .2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-------------+\n",
      "| id|        fullName|         City|\n",
      "+---+----------------+-------------+\n",
      "|  0|Herman Zimmerman|Oklahoma City|\n",
      "|  1|        Lisa Ray|     Columbus|\n",
      "|  2|  Terrell Reeves| Jacksonville|\n",
      "|  3|   Steve Goodwin|    Charlotte|\n",
      "|  4|       Leah Tran|      Detroit|\n",
      "+---+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ffdf = spark.read.csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/CDC_data/final_output/finalFile.csv\")\n",
    "ffdf = fldf.withColumnRenamed(\"_c0\", \"id\").\\\n",
    "        withColumnRenamed(\"_c1\", \"fullName\").\\\n",
    "            withColumnRenamed(\"_c2\", \"City\")\n",
    "ffdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Procesamos el archivo __Replication onGoing__ por cada tipo de Operacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(action='U', id='7', fullName='ABC XYZ', city='Phoenix') - <class 'pyspark.sql.types.Row'>\n",
      "Update\n",
      "Row(action='I', id='130', fullName='Alica Bing', city='New York') - <class 'pyspark.sql.types.Row'>\n",
      "Insert\n",
      "Row(action='I', id='131', fullName='Malinda Bing', city='Detroit') - <class 'pyspark.sql.types.Row'>\n",
      "Insert\n",
      "Row(action='I', id='132', fullName='Chandler Bing', city='Portland') - <class 'pyspark.sql.types.Row'>\n",
      "Insert\n",
      "Row(action='U', id='8', fullName='ABC XYZ', city='Denver') - <class 'pyspark.sql.types.Row'>\n",
      "Update\n",
      "Row(action='D', id='10', fullName='Jack Hicks', city='Houston') - <class 'pyspark.sql.types.Row'>\n",
      "Delete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for row in udf.collect():\n",
    "    print(\"{} - {}\".format(row, type(row)))\n",
    "    if row['action'] == 'U':\n",
    "        print('Update')\n",
    "    elif row['action'] == 'D':\n",
    "        print('Delete')\n",
    "    else:\n",
    "        print('Insert')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
