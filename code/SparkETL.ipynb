{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL\n",
    "## with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ETLtest\").\\\n",
    "    config(\"spark.jars\", \"postgresql-42.7.3.jar\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|This is a Japanes...|\n",
      "|The team members ...|\n",
      "|As the years pass...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/etl_data/WordData.txt\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|                 _c0|         splitedData|splitedDataExplode|\n",
      "+--------------------+--------------------+------------------+\n",
      "|This is a Japanes...|[This, is, a, Jap...|              This|\n",
      "|This is a Japanes...|[This, is, a, Jap...|                is|\n",
      "|This is a Japanes...|[This, is, a, Jap...|                 a|\n",
      "|This is a Japanes...|[This, is, a, Jap...|          Japanese|\n",
      "+--------------------+--------------------+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"splitedData\", f.split(col(\"_c0\"), \" \"))\n",
    "df = df.withColumn(\"splitedDataExplode\", f.explode(col(\"splitedData\")))\n",
    "df.show(4)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|splitedDataExplode|count|\n",
      "+------------------+-----+\n",
      "|          Tomorrow|    4|\n",
      "|                If|    8|\n",
      "|             leave|    4|\n",
      "|             corny|    4|\n",
      "+------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group = df.groupBy(\"splitedDataExplode\").count()\n",
    "df_group.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load - Conexion con BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"org.postgresql.Driver\"\n",
    "url = \"jdbc:postgresql://localhost:5432/ny_taxy\"\n",
    "user = \"root\"\n",
    "password = \"root\"\n",
    "db = \"ny_taxy\"\n",
    "tables = \"mi_spark.tabla_aux\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group.write.format(\"jdbc\").option(\"driver\", driver).\\\n",
    "    option(\"url\", url).\\\n",
    "        option(\"dbtable\", tables).\\\n",
    "            mode(\"append\").\\\n",
    "                option(\"user\", user).\\\n",
    "                    option(\"password\", password).\\\n",
    "                        save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Leemos de una BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_df = spark.read.format(\"jdbc\").option(\"driver\", driver).\\\n",
    "    option(\"user\", user).\\\n",
    "        option(\"password\", password).\\\n",
    "            option(\"url\", url).\\\n",
    "                option(\"dbtable\", \"mi_spark.tabla_aux\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|splitedDataExplode|sum(count)|\n",
      "+------------------+----------+\n",
      "|          Tomorrow|        16|\n",
      "+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rd_df.filter(col(\"splitedDataExplode\")==\"Tomorrow\").groupby(col(\"splitedDataExplode\")).sum(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load - Conexi√≥n AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"org.postgresql.Driver\"\n",
    "url = \"jdbc:postgresql://mipruebapyspark.cdqe2q4c2zqa.sa-east-1.rds.amazonaws.com/\"\n",
    "user = \"postgres\"\n",
    "password = \"\"\n",
    "db = \"ny_taxy\"\n",
    "tables = \"my_conexion_spark.tabla_aws\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group.write.format(\"jdbc\").option(\"driver\", driver).\\\n",
    "    option(\"url\", url).\\\n",
    "        option(\"dbtable\", tables).\\\n",
    "            mode(\"append\").\\\n",
    "                option(\"user\", user).\\\n",
    "                    option(\"password\", password).\\\n",
    "                        save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Leemos de AWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_df = spark.read.format(\"jdbc\").option(\"driver\", driver).\\\n",
    "    option(\"user\", user).\\\n",
    "        option(\"password\", password).\\\n",
    "            option(\"url\", url).\\\n",
    "                option(\"dbtable\", \"my_conexion_spark.tabla_aws\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|splitedDataExplode|sum(count)|\n",
      "+------------------+----------+\n",
      "|          Tomorrow|         4|\n",
      "+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rd_df.filter(col(\"splitedDataExplode\")==\"Tomorrow\").\\\n",
    "    groupby(col(\"splitedDataExplode\")).\\\n",
    "        sum(\"count\").\\\n",
    "            show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
