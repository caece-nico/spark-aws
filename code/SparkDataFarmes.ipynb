{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creamos una SparSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/01 12:32:42 WARN Utils: Your hostname, DESKTOP-SLEQT56 resolves to a loopback address: 127.0.1.1; using 172.25.13.138 instead (on interface eth0)\n",
      "24/04/01 12:32:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/01 12:32:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "            .appName('Test')\\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Leemos un archivo .CSV para crear un __DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/mnt/d/Proyectos/Tutorial-SparkAWS/data/StudentData.csv\"\n",
    "df = spark.read\\\n",
    "    .option(\"header\", \"True\") \\\n",
    "        .csv(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Aplicamos una __Accion__sobre el __DataFrame__ para mostrar su contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...|\n",
      "| 29|Female|    Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|  Sheryll Towler|   DSA|41487|   41|Claude Panos_Judi...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Trabajamos con Esquemas\n",
    "\n",
    "2.1 Mostramos el esquema de DF anterior __Vemos que todo es string__ y no es lo que queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- roll: string (nullable = true)\n",
      " |-- marks: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Inferimos el Esquema usando __\".option(\"inferSchema\", True)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- roll: integer (nullable = true)\n",
      " |-- marks: integer (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"Header\",\"True\").option(\"inferSchema\", True).csv(PATH)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Especificar esquema de forma manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...|\n",
      "| 29|Female|    Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|  Sheryll Towler|   DSA|41487|   41|Claude Panos_Judi...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- roll: string (nullable = true)\n",
      " |-- marks: integer (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType \n",
    "\n",
    "schema_a = StructType ([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"course\", StringType(), True),\n",
    "    StructField(\"roll\", StringType(), True),\n",
    "    StructField(\"marks\", IntegerType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema_a).option(\"header\",True).csv(PATH)\n",
    "df.show(5)\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Usar **Kwargs en Spark\n",
    "\n",
    "Se usa para no tener tantos __.option()__ anidados remplazandolo por un unico __.options()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB| 2984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(inferSchema=True, Header=True, delimiter=\",\").csv(PATH)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Spark DF from RDDs\n",
    "\n",
    "Primero creamos un RDD y Luego lo pasamos a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('Test_rdd').setMaster(\"local[*]\")\n",
    "spark_rdd = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/mnt/d/Proyectos/Tutorial-SparkAWS/data/StudentData.csv\"\n",
    "\n",
    "rdd = spark_rdd.textFile(PATH)\n",
    "header = rdd.first()\n",
    "rdd_data = rdd.filter(lambda x: x != header)\n",
    "#print(rdd_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28,\n",
       " 'Female',\n",
       " 'Hubert Oliveras',\n",
       " 'DB',\n",
       " '02984',\n",
       " 59,\n",
       " 'Annika Hoffman_Naoma Fritts@OOP.com']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data_split = rdd_data.map(lambda x:x.split(\",\"))\n",
    "rdd_data_f = rdd_data_split.map(lambda x: [int(x[0]), x[1], x[2], x[3], x[4], int(x[5]), x[6]])\n",
    "rdd_data_f.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Convertimos el __Rdd__ en un __DataFrame__ usando el Header que sacamos el __Rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_headers = header.split(\",\")\n",
    "df = rdd_data_f.toDF(lista_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...|\n",
      "| 29|Female|    Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|  Sheryll Towler|   DSA|41487|   41|Claude Panos_Judi...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- roll: string (nullable = true)\n",
      " |-- marks: long (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Como el esquema no se puede inferir, lo que hacemos es crear nuestro propio esquema, pero en lugar de usar la __transformacion rdd.toDF()__ usamos __spark.createDataFrame(rdd, schema=)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- roll: string (nullable = true)\n",
      " |-- marks: integer (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "schema_rdd = StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"course\", StringType(), True),\n",
    "    StructField(\"roll\", StringType(), True),\n",
    "    StructField(\"marks\", IntegerType(), True),\n",
    "    StructField(\"email\", StringType(), True)\n",
    "])\n",
    "\n",
    "mi_df = spark.createDataFrame(rdd_data_f, schema=schema_rdd)\n",
    "print(mi_df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Seleccion de columnas\n",
    "\n",
    "5.1 Notación directa, especificando la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|gender|\n",
      "+---+------+\n",
      "| 28|Female|\n",
      "| 29|Female|\n",
      "+---+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mi_df.select(\"age\", \"gender\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Usando __dot notation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|age|               email|\n",
      "+---+--------------------+\n",
      "| 28|Annika Hoffman_Na...|\n",
      "| 29|Margene Moores_Ma...|\n",
      "+---+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.age, df.email).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Usando __col()__\n",
    "\n",
    "Es una funcion dentro de la libreria __pyspark.sql.functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|gender|\n",
      "+---+------+\n",
      "| 28|Female|\n",
      "| 29|Female|\n",
      "| 28|  Male|\n",
      "| 29|Female|\n",
      "| 28|  Male|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select(col(\"age\"), col(\"gender\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Mostrar todas las columnas usando __*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...|\n",
      "| 29|Female|    Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5 Select de las columnas usando Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+--------------------+\n",
      "|course| roll|marks|               email|\n",
      "+------+-----+-----+--------------------+\n",
      "|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "|    PF|21267|   45|Jeannetta Golden_...|\n",
      "|    DB|32877|   29|Billi Clore_Mitzi...|\n",
      "+------+-----+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns[3:]).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6 Podemos combinar todo lo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|age|               email|\n",
      "+---+--------------------+\n",
      "| 28|Annika Hoffman_Na...|\n",
      "| 29|Margene Moores_Ma...|\n",
      "| 28|Jeannetta Golden_...|\n",
      "| 29|Billi Clore_Mitzi...|\n",
      "+---+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"age\"), df.columns[-1]).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. WithColumn()\n",
    "\n",
    "6.1 Ejemplo de Casteo de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+-----+\n",
      "|age|gender|            name|course| roll|marks|               email|roll2|\n",
      "+---+------+----------------+------+-----+-----+--------------------+-----+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...| 2984|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|12899|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...|21267|\n",
      "| 29|Female|    Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...|32877|\n",
      "| 28|  Male|  Sheryll Towler|   DSA|41487|   41|Claude Panos_Judi...|41487|\n",
      "+---+------+----------------+------+-----+-----+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"roll2\", col(\"roll\").cast(\"Integer\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Ejemplo de manipulacion de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+------+\n",
      "|age|gender|            name|course| roll|marks|               email|marks2|\n",
      "+---+------+----------------+------+-----+-----+--------------------+------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|    64|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|    67|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...|    50|\n",
      "| 29|Female|    Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...|    34|\n",
      "| 28|  Male|  Sheryll Towler|   DSA|41487|   41|Claude Panos_Judi...|    46|\n",
      "+---+------+----------------+------+-----+-----+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.withColumn(\"marks2\", col(\"marks\") + 5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3 Crear un valor constante\n",
    "\n",
    "Con .withColumn se puede crear un valor constante, pero hay que __especificarlo con la funcion lit de .sql.functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+----+\n",
      "|age|gender|            name|course| roll|marks|               email|pais|\n",
      "+---+------+----------------+------+-----+-----+--------------------+----+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...| ARG|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...| ARG|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...| ARG|\n",
      "| 29|Female|    Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...| ARG|\n",
      "| 28|  Male|  Sheryll Towler|   DSA|41487|   41|Claude Panos_Judi...| ARG|\n",
      "+---+------+----------------+------+-----+-----+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "df.withColumn('pais',lit('ARG')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4 Concatenacion de .withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   69|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   72|Margene Moores_Ma...|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   55|Jeannetta Golden_...|\n",
      "| 29|Female|    Elenore Choy|    DB|32877|   39|Billi Clore_Mitzi...|\n",
      "| 28|  Male|  Sheryll Towler|   DSA|41487|   51|Claude Panos_Judi...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.withColumn('marks', col('marks') - 10).withColumn('marks', col('marks') + 20).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. withColumnRename\n",
    "\n",
    "Lo usamos para cambiar el nombre de una columna y generar un nuevo __DF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----------------+------+-----+-----+--------------------+\n",
      "|edad|gender|            name|course| roll|marks|               email|\n",
      "+----+------+----------------+------+-----+-----+--------------------+\n",
      "|  28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "|  29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "|  28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...|\n",
      "|  29|Female|    Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...|\n",
      "+----+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rename = df.withColumnRenamed(\"age\", \"edad\")\n",
    "df_rename.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 Si queremos cambiar el nombre de una variable en tiempo de ejecución usamos __.alias()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|Age|\n",
      "+---+\n",
      "| 28|\n",
      "| 29|\n",
      "| 28|\n",
      "| 29|\n",
      "| 28|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_rename.select(col(\"edad\").alias(\"Age\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Filter en __DataFrames__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+------+------+-----+--------------------+\n",
      "|age|gender|           name|course|  roll|marks|               email|\n",
      "+---+------+---------------+------+------+-----+--------------------+\n",
      "| 28|Female|Hubert Oliveras|    DB| 02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|   Elenore Choy|    DB| 32877|   29|Billi Clore_Mitzi...|\n",
      "| 29|  Male|Ernest Rossbach|    DB|111449|   53|Maybell Duguay_Ab...|\n",
      "| 28|Female| Latia Vanhoose|    DB|122502|   27|Latia Vanhoose_Mi...|\n",
      "| 29|Female| Latia Vanhoose|    DB|152159|   27|Claude Panos_Sant...|\n",
      "+---+------+---------------+------+------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+------+---------------+------+------+-----+--------------------+\n",
      "|age|gender|           name|course|  roll|marks|               email|\n",
      "+---+------+---------------+------+------+-----+--------------------+\n",
      "| 28|Female|Hubert Oliveras|    DB| 02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|   Elenore Choy|    DB| 32877|   29|Billi Clore_Mitzi...|\n",
      "| 29|  Male|Ernest Rossbach|    DB|111449|   53|Maybell Duguay_Ab...|\n",
      "| 28|Female| Latia Vanhoose|    DB|122502|   27|Latia Vanhoose_Mi...|\n",
      "| 29|Female| Latia Vanhoose|    DB|152159|   27|Claude Panos_Sant...|\n",
      "+---+------+---------------+------+------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"course\")=='DB').show(5)\n",
    "df.filter(df.course == \"DB\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2 Condición multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "|age|gender|            name|course|  roll|marks|               email|\n",
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB| 02984|   59|Annika Hoffman_Na...|\n",
      "| 29|  Male| Ernest Rossbach|    DB|111449|   53|Maybell Duguay_Ab...|\n",
      "| 28|Female|Mickey Cortright|    DB|192537|   62|Ernest Rossbach_M...|\n",
      "| 28|Female|     Anna Santos|    DB|311589|   79|Celeste Lollis_Mi...|\n",
      "| 29|  Male|    Paris Hutton|    DB|481229|   57|Clementina Menke_...|\n",
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\n",
    "            (df.course == \"DB\") & (df.marks > 50)\n",
    "        ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.3 Funcionalidad __isin()__ para condiciones múltiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "l_courses = [\"DB\", \"Cloud\", \"OOP\"]\n",
    "df.filter(col(\"course\").isin(l_courses)).show(2)\n",
    "df.filter(df.course.isin(l_courses)).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.4 Condición __startsWith()_\n",
    "\n",
    "La usamos para determinar cuando una cadena comienza con una determinada letra co caracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+------+-----+-----+--------------------+\n",
      "|age|gender|           name|course| roll|marks|               email|\n",
      "+---+------+---------------+------+-----+-----+--------------------+\n",
      "| 28|Female|Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|   Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...|\n",
      "+---+------+---------------+------+-----+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---+------+---------------+------+-----+-----+--------------------+\n",
      "|age|gender|           name|course| roll|marks|               email|\n",
      "+---+------+---------------+------+-----+-----+--------------------+\n",
      "| 28|Female|Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|   Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...|\n",
      "+---+------+---------------+------+-----+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(df.course.startswith(\"D\")).show(2)\n",
    "df.filter(col(\"course\").startswith(\"D\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.5 Filtro con __.contains()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+------+------+-----+--------------------+\n",
      "|age|gender|        name|course|  roll|marks|               email|\n",
      "+---+------+------------+------+------+-----+--------------------+\n",
      "| 29|Female|Elenore Choy|    DB| 32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|Elenore Choy|    PF|422704|   66|Marylee Capasso_N...|\n",
      "+---+------+------------+------+------+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---+------+------------+------+------+-----+--------------------+\n",
      "|age|gender|        name|course|  roll|marks|               email|\n",
      "+---+------+------------+------+------+-----+--------------------+\n",
      "| 29|Female|Elenore Choy|    DB| 32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|Elenore Choy|    PF|422704|   66|Marylee Capasso_N...|\n",
      "+---+------+------------+------+------+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.contains(\"Eleno\")).show(2)\n",
    "df.filter(col(\"name\").contains(\"Eleno\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.6 Filtro con __.like()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+------+------+-----+--------------------+\n",
      "|age|gender|        name|course|  roll|marks|               email|\n",
      "+---+------+------------+------+------+-----+--------------------+\n",
      "| 29|Female|Elenore Choy|    DB| 32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|Elenore Choy|    PF|422704|   66|Marylee Capasso_N...|\n",
      "+---+------+------------+------+------+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---+------+------------+------+------+-----+--------------------+\n",
      "|age|gender|        name|course|  roll|marks|               email|\n",
      "+---+------+------------+------+------+-----+--------------------+\n",
      "| 29|Female|Elenore Choy|    DB| 32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|Elenore Choy|    PF|422704|   66|Marylee Capasso_N...|\n",
      "+---+------+------------+------+------+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(df.name.like(\"%Ele%\")).show(2)\n",
    "df.filter(col(\"name\").like(\"%Ele%\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Spark Ejercicio rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql .types import StructField, StructType, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"course\", StringType(), True),\n",
    "    StructField(\"roll\", StringType(), True),\n",
    "    StructField(\"marks\", IntegerType(), True),\n",
    "    StructField(\"emails\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(header=True, schema=schema).csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/StudentData.csv\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Crear una nueva columna __total marks__ con el valor 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"totalMarks\", lit(120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Crear una columna __total mark avg__ para cada alumno con la formula \n",
    "__(marks/total_marks)*100__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"avg\", col(\"marks\")/col(\"totalMarks\") * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Filtrar los estudiantes con AVG > 80% in __OOP__\n",
    "4. Filtrar los estudiantes con AVG > 60% in __CLOUD__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oop = df.filter(\n",
    "                    (df.avg>=80) & (df.course == \"OOP\") \n",
    "                )\n",
    "                 \n",
    "df_cloud =  df.filter(               \n",
    "                    (col(\"avg\")>= 60) & (col(\"course\")==\"Cloud\")\n",
    "                    )\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Mostrar los nombres y notas de __3__ y __4__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|          name|marks|\n",
      "+--------------+-----+\n",
      "| Kizzy Brenner|   96|\n",
      "|Jenna Montague|   98|\n",
      "+--------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------+-----+\n",
      "|        name|marks|\n",
      "+------------+-----+\n",
      "|Claude Panos|   85|\n",
      "| Billi Clore|   76|\n",
      "+------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_oop.select(col(\"name\"), col(\"marks\")).show(2)\n",
    "df_cloud.select(col(\"name\"), col(\"marks\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Funciones SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.1 Distinct y count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|gender|age|\n",
      "+------+---+\n",
      "|  Male| 29|\n",
      "|  Male| 28|\n",
      "|Female| 28|\n",
      "|Female| 29|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"gender\", \"age\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2 dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+----------+------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|totalMarks|               avg|\n",
      "+---+------+----------------+------+-----+-----+--------------------+----------+------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|       120|49.166666666666664|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|       120| 51.66666666666667|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...|       120|              37.5|\n",
      "| 29|  Male|  Cordie Harnois|   OOP|92882|   51|Judie Chipps_Clem...|       120|              42.5|\n",
      "+---+------+----------------+------+-----+-----+--------------------+----------+------------------+\n",
      "\n",
      "+---+------+----------------+------+-----+-----+--------------------+----------+------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|totalMarks|               avg|\n",
      "+---+------+----------------+------+-----+-----+--------------------+----------+------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|       120|49.166666666666664|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|       120| 51.66666666666667|\n",
      "+---+------+----------------+------+-----+-----+--------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates([\"gender\",\"age\"]).show()\n",
    "df.dropDuplicates([\"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Spark Ejercicio rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- roll: string (nullable = true)\n",
      " |-- marks: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"course\", StringType(), True),\n",
    "    StructField(\"roll\", StringType(), True),\n",
    "    StructField(\"marks\", IntegerType(), True),\n",
    "    StructField(\"emails\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.options(schema=schema, header=True).csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/StudentData.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solucion 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.select(\"age\", \"gender\", \"course\").distinct()\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solucion 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropDuplicates([\"gender\", \"age\", \"course\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. .sort() u .orderBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------+------+-------+-----+--------------------+\n",
      "|age|gender|          name|course|   roll|marks|               email|\n",
      "+---+------+--------------+------+-------+-----+--------------------+\n",
      "| 28|Female|Melani Engberg| Cloud|1872667|   99|Alberta Freund_Ni...|\n",
      "| 28|  Male|   Niki Klimek|   DSA|5172507|   99|Marylee Capasso_E...|\n",
      "| 28|Female|  Judie Chipps|   OOP|5451977|   99|Tamera Blakley_Mi...|\n",
      "| 28|Female|Jalisa Swenson|   MVC|5712033|   99|Dustin Feagins_Hu...|\n",
      "+---+------+--------------+------+-------+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col('age').asc(), col('marks').desc()).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Spark ejercicio rapido - Sort() u orderBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#employee_name,department,state,salary,age,bonus\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('employee_name', StringType(), True),\n",
    "    StructField('department', StringType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True),\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('bonus', IntegerType(), True),\n",
    "])\n",
    "\n",
    "df_emp = spark.read.options(schema=schema, header = True).csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/OfficeData.csv\")\n",
    "df_emp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Crear un DF ordernado por la columna __bonus__ de forma ascendente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.select(\"*\").sort(col(\"bonus\").asc()).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Crear un DF ordenado por las columnas __age__ y __salary__ en orden descendente y ascendente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.select(\"*\").sort(col(\"age\").desc(), col(\"salary\").asc()).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Crear un DF ordenado por __age__ __bonus__ y __salario__ en orden descendente, descentente y ascendente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.select(\"*\").sort(col(\"age\").desc(), col(\"bonus\").desc(), col(\"salary\").asc()).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. GroupBy en DF\n",
    "\n",
    "14.1 Unica columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- roll: string (nullable = true)\n",
      " |-- marks: integer (nullable = true)\n",
      " |-- emails: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "\n",
    "schemas = StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"course\", StringType(), True),\n",
    "    StructField(\"roll\", StringType(), True),\n",
    "    StructField(\"marks\", IntegerType(), True),\n",
    "    StructField(\"emails\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schemas).option(\"header\",\"True\").csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/StudentData.csv\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|Female|  501|\n",
      "|  Male|  499|\n",
      "+------+-----+\n",
      "\n",
      "+------+----------+\n",
      "|gender|sum(marks)|\n",
      "+------+----------+\n",
      "|Female|     29636|\n",
      "|  Male|     30461|\n",
      "+------+----------+\n",
      "\n",
      "+------+------------------+\n",
      "|gender|        avg(marks)|\n",
      "+------+------------------+\n",
      "|Female|59.153692614770456|\n",
      "|  Male| 61.04408817635271|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"gender\").count().show()\n",
    "df.groupBy(\"gender\").sum(\"marks\").show()\n",
    "df.groupBy(\"gender\").avg(\"marks\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.2 groupBy en mas de una columna y agregaciones multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+----------+----------+\n",
      "|gender|course|count(1)|min(marks)|sum(marks)|\n",
      "+------+------+--------+----------+----------+\n",
      "|Female| Cloud|     106|        20|      6316|\n",
      "|Female|   OOP|      82|        21|      4682|\n",
      "|  Male|    PF|      97|        20|      5960|\n",
      "|  Male|    DB|      82|        20|      5073|\n",
      "+------+------+--------+----------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum, avg, max, min, mean, count\n",
    "\n",
    "\n",
    "df.groupBy(col(\"gender\"), col(\"course\"))\\\n",
    "    .agg(count(\"*\"),\\\n",
    "        min(\"marks\"), \\\n",
    "            sum(col(\"marks\"))).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.3 groupBy  + Alias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------+-----------+----------+\n",
      "|gender|age|cantidad_total|nota_minima|suma_notas|\n",
      "+------+---+--------------+-----------+----------+\n",
      "|Female| 29|           245|         20|     14419|\n",
      "|Female| 28|           256|         20|     15217|\n",
      "|  Male| 28|           238|         20|     14664|\n",
      "|  Male| 29|           261|         20|     15797|\n",
      "+------+---+--------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(col(\"gender\"), col(\"age\"))\\\n",
    "    .agg(count(\"*\").alias(\"cantidad_total\")\\\n",
    "        ,min(\"marks\").alias(\"nota_minima\")\\\n",
    "            , sum(col(\"marks\")).alias(\"suma_notas\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. GroupBy() y Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el agrupado siempre se hace despues del __.filter()__ con los registros resultantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|Female|     399|\n",
      "|  Male|     410|\n",
      "+------+--------+\n",
      "\n",
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|Female|     102|\n",
      "|  Male|      89|\n",
      "+------+--------+\n",
      "\n",
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|Female|     501|\n",
      "|  Male|     499|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"marks\") > 35).groupBy(col(\"gender\")).agg(count(\"*\")).show()\n",
    "df.filter(col(\"marks\") <= 35).groupBy(col(\"gender\")).agg(count(\"*\")).show()\n",
    "df.groupBy(col(\"gender\")).agg(count(\"*\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.1 Concatenacion y agrupados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------------+\n",
      "|gender|age|total_gender|\n",
      "+------+---+------------+\n",
      "|Female| 29|         194|\n",
      "|Female| 28|         205|\n",
      "|  Male| 28|         197|\n",
      "|  Male| 29|         213|\n",
      "+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aux = df.filter(col(\"marks\") > 35).groupBy(col(\"gender\"),col(\"age\")).agg(count(\"*\").alias(\"total_gender\"))\n",
    "df_aux.filter(df_aux.total_gender > 50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------------+\n",
      "|gender|age|total_gender|\n",
      "+------+---+------------+\n",
      "|Female| 29|         194|\n",
      "|Female| 28|         205|\n",
      "|  Male| 28|         197|\n",
      "|  Male| 29|         213|\n",
      "+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"marks\") > 35).groupBy(col(\"gender\"),col(\"age\")).agg(count(\"*\").alias(\"total_gender\")).filter(col(\"total_gender\") > 50).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Spark ejercicio rápido GroupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- roll: string (nullable = true)\n",
      " |-- marks: integer (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max, min, avg, count, sum\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType\n",
    "\n",
    "#age,gender,name,course,roll,marks,email\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"age\",IntegerType(), True),\n",
    "    StructField(\"gender\",StringType(), True),\n",
    "    StructField(\"name\",StringType(), True),\n",
    "    StructField(\"course\",StringType(), True),\n",
    "    StructField(\"roll\",StringType(), True),\n",
    "    StructField(\"marks\",IntegerType(), True),\n",
    "    StructField(\"email\",StringType(), True)\n",
    "    \n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).option(\"header\",\"True\").csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/StudentData.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...|\n",
      "| 29|Female|    Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|  Sheryll Towler|   DSA|41487|   41|Claude Panos_Judi...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. El numero total de estudiantes en cada curso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|course|Total_alumnos|\n",
      "+------+-------------+\n",
      "| Cloud|          192|\n",
      "|   DSA|          176|\n",
      "+------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(col(\"course\"))\\\n",
    "    .agg(count('*')\\\n",
    "        .alias('Total_alumnos'))\\\n",
    "            .sort(col(\"Total_alumnos\").desc())\\\n",
    "                .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. EL numero total de H y M en cada curso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "|course|gender|total|\n",
      "+------+------+-----+\n",
      "|    PF|Female|   69|\n",
      "|   OOP|  Male|   70|\n",
      "|   MVC|Female|   71|\n",
      "|    DB|Female|   75|\n",
      "+------+------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(col(\"course\"), col(\"gender\"))\\\n",
    "    .agg(count(\"*\")\\\n",
    "        .alias(\"total\"))\\\n",
    "            .sort(col(\"total\").asc())\\\n",
    "                .show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. EL total de notas por cada genero en cada curso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+\n",
      "|course|gender|Suma_total|\n",
      "+------+------+----------+\n",
      "|   OOP|  Male|      4234|\n",
      "|    DB|  Male|      5073|\n",
      "| Cloud|Female|      6316|\n",
      "|   MVC|  Male|      5241|\n",
      "+------+------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(col(\"course\"), col(\"gender\")).agg(sum(col(\"marks\")).alias(\"Suma_total\")).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Por cada curso, el maximo, minimo y AVG por grupo de edad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-------+-------+\n",
      "|course|           avg_age|max_age|min_age|\n",
      "+------+------------------+-------+-------+\n",
      "|    PF| 28.52409638554217|     29|     28|\n",
      "|    DB|28.477707006369428|     29|     28|\n",
      "+------+------------------+-------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(col(\"course\"))\\\n",
    "    .agg(avg(col(\"age\")).alias(\"avg_age\")\\\n",
    "        , max(col(\"age\")).alias(\"max_age\") \\\n",
    "            , min(col(\"age\")).alias(\"min_age\"))\\\n",
    "                .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Spark ejercicio rápido WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| Word|\n",
      "+-----+\n",
      "|Apple|\n",
      "|  Mic|\n",
      "|  Mic|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, sum, min, max, count, col\n",
    "from pyspark.sql.types import StructField, StructType\n",
    "\n",
    "header = ['Word']\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Word\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_words = spark.read.option(\"header\", False).schema(schema)\\\n",
    "    .csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/WordData.txt\")\n",
    "df_words.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  Word|Total|\n",
      "+------+-----+\n",
      "|  Book|    5|\n",
      "|Mobile|    5|\n",
      "|Laptop|    5|\n",
      "|   Bag|    5|\n",
      "|   Mic|   10|\n",
      "| Apple|   10|\n",
      "| Chair|   15|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_words.groupBy('Word').agg(count(\"*\").alias(\"Total\")).sort(col(\"Total\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. UDF\n",
    "\n",
    "Creamos una funcion que devuelva en una nueva columna la suma entre __salario__ y __bonus__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- bonus: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf, lit\n",
    "\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/OfficeData.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+-------------+\n",
      "|employee_name|department|state|salary|age|bonus|total_salario|\n",
      "+-------------+----------+-----+------+---+-----+-------------+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|       100000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|       106000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|       104000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|       113000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|       123000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|       102000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|        94000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|        98000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|       112000|\n",
      "+-------------+----------+-----+------+---+-----+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def get_total_slary(salary, bonus):\n",
    "    return salary + bonus\n",
    "\n",
    "getTotalSalaryUDF = udf(lambda x,y: get_total_slary(x,y), IntegerType())\n",
    "\n",
    "df.withColumn(\"total_salario\", getTotalSalaryUDF(col(\"salary\"), col(\"bonus\")) ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Spark Ejercicio rapido - UDF .withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- bonus: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/OfficeData.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Crear una nueva columna que muestre el incremento en el sueldo segun:\n",
    "- Si el empleado esta en NY, 10% en salario y 5% en bonus\n",
    "- Si esta en CA, 12% en salario y 3% en bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+-----------------+\n",
      "|employee_name|department|state|salary|age|bonus|incremento_salary|\n",
      "+-------------+----------+-----+------+---+-----+-----------------+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|         114000.0|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|         119900.0|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|         114410.0|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|         124490.0|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|         135600.0|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|         115400.0|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|         106600.0|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|         108140.0|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|         126700.0|\n",
      "+-------------+----------+-----+------+---+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def increment_salary(city, salary, bonus):\n",
    "    suma = 0\n",
    "    if city == \"NY\":\n",
    "        suma = salary * 1.15 + bonus * 1.05\n",
    "    elif city == \"CA\":\n",
    "        suma = salary * 1.12 + bonus *1.03\n",
    "        \n",
    "    return suma\n",
    "\n",
    "\n",
    "get_salary = udf(lambda x,y,z: increment_salary(x,y,z), FloatType())\n",
    "\n",
    "df.withColumn(\"incremento_salary\", get_salary(col(\"state\"), col(\"salary\"), col(\"bonus\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Cache y Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StructField, StringType, StructType\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_df = StructType([\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"gender\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"course\", StringType(), False),\n",
    "    StructField(\"roll\", StringType(), False),\n",
    "    StructField(\"marks\", IntegerType(), False),\n",
    "    StructField(\"email\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...|\n",
      "| 29|Female|    Elenore Choy|    DB|32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|  Sheryll Towler|   DSA|41487|   41|Claude Panos_Judi...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\",\"True\").schema(schema_df).csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/StudentData.csv\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- roll: string (nullable = true)\n",
      " |-- marks: integer (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este ejemplo aplicamos una dos transformaciones, hacemos __cache__ sobre ese __dataframe__ y luego una accion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trx = df.groupBy(\"gender\", \"course\", \"age\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trx = df_trx.withColumn(\"dummy\", col(\"age\") + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[gender: string, course: string, age: int, count: bigint, dummy: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trx.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+-----+-----+\n",
      "|gender|course|age|count|dummy|\n",
      "+------+------+---+-----+-----+\n",
      "|Female|   OOP| 29|   39|  129|\n",
      "|  Male|   MVC| 28|   38|  128|\n",
      "|Female|   MVC| 28|   34|  128|\n",
      "|Female|    DB| 28|   40|  128|\n",
      "|  Male|    PF| 29|   53|  129|\n",
      "+------+------+---+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trx.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__AGREGAR CONVERTIR A RDD__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "|age|gender|            name|course| roll|marks|               email|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|02984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud|12899|   62|Margene Moores_Ma...|\n",
      "| 28|  Male|  Celeste Lollis|    PF|21267|   45|Jeannetta Golden_...|\n",
      "+---+------+----------------+------+-----+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mi_df = spark.read.\\\n",
    "    option(\"header\",\"true\").\\\n",
    "        schema(schema_df).\\\n",
    "            csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/StudentData.csv\")\n",
    "            \n",
    "mi_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+\n",
      "|gender|age|count(1)|\n",
      "+------+---+--------+\n",
      "|Female| 29|     245|\n",
      "|Female| 28|     256|\n",
      "|  Male| 28|     238|\n",
      "|  Male| 29|     261|\n",
      "+------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mi_df.createOrReplaceTempView(\"miVista\")\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "               select gender, age, count(*)\n",
    "               from miVista\n",
    "               group by gender, age\n",
    "               \"\"\")\n",
    "\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. Write DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.option(\"header\", \"true\").csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/agrupados/alumnos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. Trabajo integrador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min, max, avg, udf\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType, StructType, StructField\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/01 12:33:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    master(\"local\").\\\n",
    "        appName(\"TrabajoIntegrador\").\\\n",
    "            getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"employee_name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"salary\", FloatType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"bonus\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+----------+-----+------+---+-----+\n",
      "|employee_id|    employee_name|department|state|salary|age|bonus|\n",
      "+-----------+-----------------+----------+-----+------+---+-----+\n",
      "|       1000|        Nitz Leif| Marketing|   CA|6131.0| 26|  543|\n",
      "|       1001|  Melissia Dedman|   Finance|   AK|4027.0| 43| 1290|\n",
      "|       1002|Rudolph Barringer|        HR|   LA|3122.0| 43| 1445|\n",
      "|       1003|      Tamra Amber|  Accounts|   AK|5717.0| 47| 1291|\n",
      "+-----------+-----------------+----------+-----+------+---+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mi_df = spark.read.option(\"header\", \"true\").\\\n",
    "    schema(mi_schema).csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/OfficeDataProject.csv\")\n",
    "\n",
    "mi_df.createOrReplaceTempView(\"mi_df_tabla\")\n",
    "\n",
    "mi_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Número total de empleados en la compania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          select count(*)\n",
    "          from mi_df_tabla\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Número total de departamentos en la compania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_df.select(col(\"department\")).\\\n",
    "    distinct().\\\n",
    "        count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT department)|\n",
      "+--------------------------+\n",
      "|                         6|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT count(distinct department)\n",
    "          FROM mi_df_tabla\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. El nombre de los departamentos en la compania.\n",
    "\n",
    "Lo podemos hacer usando __distinct()__ o __dropDuplicates()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|department|\n",
      "+----------+\n",
      "|     Sales|\n",
      "|        HR|\n",
      "|   Finance|\n",
      "|Purchasing|\n",
      "| Marketing|\n",
      "|  Accounts|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mi_df.select(col(\"department\")).\\\n",
    "    distinct().\\\n",
    "        show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|nombre_depto|\n",
      "+------------+\n",
      "|       Sales|\n",
      "|          HR|\n",
      "|     Finance|\n",
      "|  Purchasing|\n",
      "|   Marketing|\n",
      "|    Accounts|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT DISTINCT(department) as nombre_depto\n",
    "          from mi_df_tabla\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Número de empleados por cada departamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|  169|\n",
      "|        HR|  171|\n",
      "|   Finance|  162|\n",
      "|Purchasing|  166|\n",
      "| Marketing|  170|\n",
      "|  Accounts|  162|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mi_df.groupBy(col(\"department\")).\\\n",
    "    count().\\\n",
    "        show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|department|count(1)|\n",
      "+----------+--------+\n",
      "|     Sales|     169|\n",
      "|        HR|     171|\n",
      "|   Finance|     162|\n",
      "|Purchasing|     166|\n",
      "| Marketing|     170|\n",
      "|  Accounts|     162|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT department, count(*)\n",
    "          FROM mi_df_tabla\n",
    "          GROUP BY department\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Número de empleados en cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|department|state|count|\n",
      "+----------+-----+-----+\n",
      "|  Accounts|   WA|   27|\n",
      "|  Accounts|   NY|   34|\n",
      "|  Accounts|   LA|   29|\n",
      "|  Accounts|   CA|   35|\n",
      "|  Accounts|   AK|   37|\n",
      "|   Finance|   WA|   30|\n",
      "|   Finance|   NY|   31|\n",
      "|   Finance|   LA|   29|\n",
      "|   Finance|   CA|   35|\n",
      "|   Finance|   AK|   37|\n",
      "|        HR|   WA|   47|\n",
      "|        HR|   NY|   30|\n",
      "|        HR|   LA|   41|\n",
      "|        HR|   CA|   28|\n",
      "|        HR|   AK|   25|\n",
      "| Marketing|   WA|   39|\n",
      "| Marketing|   NY|   30|\n",
      "| Marketing|   LA|   26|\n",
      "| Marketing|   CA|   33|\n",
      "| Marketing|   AK|   42|\n",
      "+----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mi_df.groupBy(col(\"department\"), col(\"state\")).count().\\\n",
    "    orderBy(col(\"department\").asc(), col(\"state\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+\n",
      "|department|state|cta|\n",
      "+----------+-----+---+\n",
      "|  Accounts|   WA| 27|\n",
      "|  Accounts|   NY| 34|\n",
      "|  Accounts|   LA| 29|\n",
      "|  Accounts|   CA| 35|\n",
      "|  Accounts|   AK| 37|\n",
      "|   Finance|   WA| 30|\n",
      "|   Finance|   NY| 31|\n",
      "|   Finance|   LA| 29|\n",
      "|   Finance|   CA| 35|\n",
      "|   Finance|   AK| 37|\n",
      "|        HR|   WA| 47|\n",
      "|        HR|   NY| 30|\n",
      "|        HR|   LA| 41|\n",
      "|        HR|   CA| 28|\n",
      "|        HR|   AK| 25|\n",
      "| Marketing|   WA| 39|\n",
      "| Marketing|   NY| 30|\n",
      "| Marketing|   LA| 26|\n",
      "| Marketing|   CA| 33|\n",
      "| Marketing|   AK| 42|\n",
      "+----------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT department, state, count(*) as cta\n",
    "          FROM mi_df_tabla\n",
    "          GROUP BY department, state\n",
    "          order by department asc, state desc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Salario máximo y mínimo en cada departamento en orden ascendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|department|min_salary|max_salary|\n",
      "+----------+----------+----------+\n",
      "|   Finance|    1006.0|    9899.0|\n",
      "|  Accounts|    1007.0|    9890.0|\n",
      "|        HR|    1013.0|    9982.0|\n",
      "| Marketing|    1031.0|    9974.0|\n",
      "|     Sales|    1103.0|    9982.0|\n",
      "|Purchasing|    1105.0|    9985.0|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mi_df.groupBy(col(\"department\")).agg(min(\"salary\").\\\n",
    "    alias('min_salary'), max(\"salary\").alias(\"max_salary\")).\\\n",
    "        orderBy(col(\"min_salary\").asc()).\\\n",
    "        show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+\n",
      "|department|sal_min|sal_max|\n",
      "+----------+-------+-------+\n",
      "|Purchasing| 1105.0| 9985.0|\n",
      "|     Sales| 1103.0| 9982.0|\n",
      "| Marketing| 1031.0| 9974.0|\n",
      "|        HR| 1013.0| 9982.0|\n",
      "|  Accounts| 1007.0| 9890.0|\n",
      "|   Finance| 1006.0| 9899.0|\n",
      "+----------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT department, min(salary) as sal_min, max(salary) as sal_max\n",
    "          FROM mi_df_tabla\n",
    "          GROUP BY department\n",
    "          ORDER BY sal_min desc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Nombre de los empleados en NY en __Finance__ cuyo bonus es mayor al bono del promedio de los empleados de NY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bonus = mi_df.filter(col(\"state\") == 'NY').\\\n",
    "    groupBy(col(\"state\")).\\\n",
    "        agg(avg(col(\"bonus\")).\n",
    "            alias(\"avg_bonus\")).\\\n",
    "                select(col(\"avg_bonus\")).\\\n",
    "                    collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explicacion__ Lo que hacemos en este caso es:\n",
    "1. Traer los registros de NY y calcular el promedio.\n",
    "2. Poner un nombre indentificativo a la columna.\n",
    "3. Seleccionamos la columna y en lugar de hacer show() hacemos __collect()__\n",
    "\n",
    "__collect()__ lo que hace es devolver el valor en forma de array donde accedemos por __indice__ y __nombre__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1251.3468208092486"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bonus[0]['avg_bonus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       employee_name|\n",
      "+--------------------+\n",
      "|       Vivan Sifford|\n",
      "|      Herder Gallman|\n",
      "|          Nena Rocha|\n",
      "|       Leif Lemaster|\n",
      "|Ellingsworth Meli...|\n",
      "|        Escoto Gilma|\n",
      "|     Georgeanna Laub|\n",
      "|     Durio Tenenbaum|\n",
      "|       Juliana Grigg|\n",
      "|        Tiffani Benz|\n",
      "|          Nitz Ilana|\n",
      "|   Phylicia Antonina|\n",
      "|         Durio Janey|\n",
      "|       Melissia Jere|\n",
      "|      Yukiko Kreamer|\n",
      "|      Nena Kensinger|\n",
      "|      Antonina Ilana|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mi_df.filter(\n",
    "                (col(\"state\")== 'NY') \n",
    "                & (col('department')=='Finance') \n",
    "                & (col('bonus') > df_bonus[0]['avg_bonus'])\n",
    "            ).\\\n",
    "                select('employee_name').\\\n",
    "                    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       employee_name|\n",
      "+--------------------+\n",
      "|       Vivan Sifford|\n",
      "|      Herder Gallman|\n",
      "|          Nena Rocha|\n",
      "|       Leif Lemaster|\n",
      "|Ellingsworth Meli...|\n",
      "|        Escoto Gilma|\n",
      "|     Georgeanna Laub|\n",
      "|     Durio Tenenbaum|\n",
      "|       Juliana Grigg|\n",
      "|        Tiffani Benz|\n",
      "|          Nitz Ilana|\n",
      "|   Phylicia Antonina|\n",
      "|         Durio Janey|\n",
      "|       Melissia Jere|\n",
      "|      Yukiko Kreamer|\n",
      "|      Nena Kensinger|\n",
      "|      Antonina Ilana|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT employee_name\n",
    "          FROM mi_df_tabla\n",
    "          WHERE bonus > (\n",
    "                            SELECT avg(bonus) as avg_bonus\n",
    "                            FROM mi_df_tabla\n",
    "                            WHERE state = 'NY'\n",
    "                            GROUP BY state\n",
    "                        ) \n",
    "            AND  department = 'Finance'\n",
    "            AND state = 'NY'\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Aumentar $500 el salario de todos los empleados con edad mayor a 45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incrementSalary(age, salary):\n",
    "    if age > 45:\n",
    "        return salary + 500\n",
    "    return salary\n",
    "\n",
    "fdxIncrementSalary = udf(lambda x,y: incrementSalary(x,y), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+----------+-----+------+---+-----+\n",
      "|employee_id|    employee_name|department|state|salary|age|bonus|\n",
      "+-----------+-----------------+----------+-----+------+---+-----+\n",
      "|       1000|        Nitz Leif| Marketing|   CA|6131.0| 26|  543|\n",
      "|       1001|  Melissia Dedman|   Finance|   AK|4027.0| 43| 1290|\n",
      "|       1002|Rudolph Barringer|        HR|   LA|3122.0| 43| 1445|\n",
      "|       1003|      Tamra Amber|  Accounts|   AK|6217.0| 47| 1291|\n",
      "+-----------+-----------------+----------+-----+------+---+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mi_df.\\\n",
    "        withColumn(\"salary\", fdxIncrementSalary(col(\"age\"), col(\"salary\"))).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+------+-------------+\n",
      "|    employee_name|age|salary|nuevo_salario|\n",
      "+-----------------+---+------+-------------+\n",
      "|        Nitz Leif| 26|6131.0|       6131.0|\n",
      "|  Melissia Dedman| 43|4027.0|       4027.0|\n",
      "|Rudolph Barringer| 43|3122.0|       3122.0|\n",
      "|      Tamra Amber| 47|5717.0|       6217.0|\n",
      "+-----------------+---+------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                employee_name, age, salary,\n",
    "                CASE WHEN age > 45 THEN salary + 500 else salary END AS nuevo_salario\n",
    "            FROM mi_df_tabla \n",
    "          \"\"\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Guardar en un DF los empleados cuya edad es >= 45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_df.filter(col(\"age\") > 45).write.csv(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/agrupados/punto10.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
