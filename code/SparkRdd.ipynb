{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creamos un contexto nuevo de Spark para leer un archivo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/29 21:19:10 WARN Utils: Your hostname, DESKTOP-SLEQT56 resolves to a loopback address: 127.0.1.1; using 172.25.13.138 instead (on interface eth0)\n",
      "24/02/29 21:19:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/29 21:19:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('readFile')\n",
    "spark = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/sampletext.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 2 23 4', '10 23 101 67', '45 65 34 56', '101 54 34 101']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sobre el __rdd__ aplicamos una __transformacion__ map que devuelve otro __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "miFile_rdd = text.map(lambda x:x.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(miFile_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2', '23', '4'],\n",
       " ['10', '23', '101', '67'],\n",
       " ['45', '65', '34', '56'],\n",
       " ['101', '54', '34', '101']]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miFile_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. En este ejemplo queremos usar una funcion __foo__ definida por el usuario donde, primero hagamos lo mismo que se hizo en __2__ pero luego se castea cada elemento a __int__ y se le suma 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 25, 6], [12, 25, 103, 69], [47, 67, 36, 58], [103, 56, 36, 103]]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mi_foo(x):\n",
    "    l =  x.split(' ')\n",
    "    ls = []\n",
    "    for num in l:\n",
    "        ls.append(int(num) + 2)\n",
    "    return ls\n",
    "\n",
    "miFile  = text.map(mi_foo)\n",
    "miFile.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio.\n",
    "\n",
    "Tenemos un archivo con saludos (texto) y debemos devolver un array con la logitud de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileText = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/wordcount.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi how are you?', 'Hope you are doing', 'great']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 3, 4], [4, 3, 3, 5], [5]]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(x):\n",
    "    array = x.split(' ')\n",
    "    l2 = []\n",
    "    for word in array:\n",
    "        l2.append(len(word))\n",
    "    return l2\n",
    "\n",
    "fileText.map(foo).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variacion del ejercicio.\n",
    "\n",
    "En lugar de usar una __UDF__ usamos una funcion __lambda__ con una __list comprehention__ para obtener el mismo resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 3, 4], [4, 3, 3, 5], [5]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText.map(lambda x: [len(word) for word in x.split(' ')]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. FlatMap\n",
    "\n",
    "Devuelve todos los elementos en un tipo de dato __unificado__  generando un nuevo __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'how', 'are', 'you?', 'Hope', 'you', 'are', 'doing', 'great']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText.flatMap(lambda x:x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. filter - lambda\n",
    "\n",
    "Se usa para segmentar el resultado de una __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.filter(lambda x: x in ('10 23 45 67', '87 54 34 101')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar filter sin __lambda function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(x):\n",
    "    if x in ('10 23 45 67', '87 54 34 101'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "text.filter(foo).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 Ejercicio 2\n",
    "\n",
    "Construir un filtro que elimine las palabras que comiencen con __a__ o __c__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this mango company animal',\n",
       " 'cat dog ant mic laptop',\n",
       " 'chair seitch mobile am charger cover',\n",
       " 'amanda any alarm ant',\n",
       " 'este es mi ejemplo']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/quiz2.txt\")\n",
    "fileText.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company', 'animal', 'cat', 'ant', 'chair', 'am', 'charger', 'cover', 'amanda', 'any', 'alarm', 'ant']\n"
     ]
    }
   ],
   "source": [
    "rdd_rts = fileText.flatMap(lambda x:x.split(' '))\\\n",
    "    .filter(lambda x: x.startswith(\"a\") or x.startswith(\"c\"))\n",
    "print(rdd_rts.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'mango', 'dog', 'mic', 'laptop', 'seitch', 'mobile', 'este', 'es', 'mi', 'ejemplo']\n"
     ]
    }
   ],
   "source": [
    "def foo(x):\n",
    "    if x.startswith(\"a\") or x.startswith(\"c\"):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "rdd_rts = fileText.flatMap(lambda x:x.split(' ')).filter(foo)\n",
    "print(rdd_rts.collect())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. RDD Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '4', '10', '45', '34', '56', '54', '2', '23', '101', '67', '65']\n"
     ]
    }
   ],
   "source": [
    "rdd_text = text.flatMap(lambda x:x.split(' ')).distinct()\n",
    "print(rdd_text.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2', '23', '4'],\n",
       " ['10', '23', '101', '67'],\n",
       " ['45', '65', '34', '56'],\n",
       " ['101', '54', '34', '101']]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.map(lambda x: x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. GroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', [1]), ('4', [1]), ('10', [1]), ('45', [1]), ('34', [1, 1]), ('56', [1]), ('54', [1]), ('2', [1]), ('23', [1, 1]), ('101', [1, 1, 1]), ('67', [1]), ('65', [1])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_split = text.flatMap(lambda x: x.split(' '))\n",
    "rdd_key_val = rdd_split.map(lambda x: (x, 1))\n",
    "rdd_reduce = rdd_key_val.groupByKey()\n",
    "rdd_mapVal = rdd_reduce.mapValues(list).collect()\n",
    "print(rdd_mapVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. reduceByKey\n",
    "\n",
    "Similar al GroupByKey pero devuelve un __rdd__ agrupado con una __operacion__ sobre los valores de las __keys__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 1), ('4', 1), ('10', 1), ('45', 1), ('34', 2), ('56', 1), ('54', 1), ('2', 1), ('23', 2), ('101', 3), ('67', 1), ('65', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_split = text.flatMap(lambda x:x.split(' '))\n",
    "rdd_map = rdd_split.map(lambda x: (x, 1))\n",
    "rdd_reduce = rdd_map.reduceByKey(lambda x,y: x + y)\n",
    "print(rdd_reduce.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Ejercicio \n",
    "\n",
    "Escribir un script que devuelva la cantidad de cada palabra contenida en el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 1), ('mango', 1), ('cat', 1), ('ant', 2), ('laptop', 1), ('chair', 1), ('mobile', 1), ('am', 1), ('este', 1), ('mi', 1), ('ejemplo', 1), ('company', 1), ('animal', 1), ('dog', 1), ('mic', 1), ('seitch', 1), ('charger', 1), ('cover', 1), ('amanda', 1), ('any', 1), ('alarm', 1), ('es', 1)]\n"
     ]
    }
   ],
   "source": [
    "textFile = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/quiz2.txt\")\n",
    "type(textFile)\n",
    "\n",
    "#textFile.collect()\n",
    "rdd_flat= textFile.flatMap(lambda x:x.split(' '))\n",
    "rdd_key_val = rdd_flat.map(lambda x: (x, 1))\n",
    "rdd_reduce = rdd_key_val.reduceByKey(lambda x, y: x + y)\n",
    "print(rdd_reduce.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Count y CountByValue\n",
    "\n",
    "Ambas son acciones que se aplican sobre __RDD__\n",
    "Count devuelve la cantidad de elementos dentro de un __rdd__\n",
    "CountByValue devuelve la cantidad de veces que una elemento aparece en un __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "23\n",
      "defaultdict(<class 'int'>, {'this': 1, 'mango': 1, 'company': 1, 'animal': 1, 'cat': 1, 'dog': 1, 'ant': 2, 'mic': 1, 'laptop': 1, 'chair': 1, 'seitch': 1, 'mobile': 1, 'am': 1, 'charger': 1, 'cover': 1, 'amanda': 1, 'any': 1, 'alarm': 1, 'este': 1, 'es': 1, 'mi': 1, 'ejemplo': 1})\n"
     ]
    }
   ],
   "source": [
    "print(textFile.count())\n",
    "print(rdd_key_val.count())\n",
    "print(rdd_flat.countByValue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ant : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for key, value in rdd_flat.countByValue().items():\n",
    "    if value > 1:\n",
    "        print (key, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Rdd saveAsTextFile\n",
    "\n",
    "Se usa para guardar un __rdd__ como un archivo de texto. POr defecto un __rdd__ está compuesto por dos particiones, asique __vamos a ver dos archivos__ con el 50% de los datos cada uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(rdd_flat.getNumPartitions())\n",
    "rdd_flat.saveAsTextFile(\"../data/miRDDTextFile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Aumentar numero de particiones\n",
    "\n",
    "Se puede usar __repartition__ para aumentar o __coalesce__ para disminuirlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "textFile = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/quiz2.txt\")\n",
    "\n",
    "\n",
    "rdd_flat = textFile.flatMap(lambda x:x.split(' '))\n",
    "print(rdd_flat.getNumPartitions())\n",
    "\n",
    "rdd_flat_4 = rdd_flat.repartition(4)\n",
    "rdd_flat_4.saveAsTextFile(\"../data/miPartition\")\n",
    "print(rdd_flat_4.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EN el ejemplo anterior vemos que se generan 4 archivos de particiones, ahora podemos usar __coalesce__ para que nos quede solo una particion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(rdd_flat_4.getNumPartitions())\n",
    "rdd_flat_coalesce = rdd_flat_4.coalesce(1)\n",
    "rdd_flat_coalesce.saveAsTextFile(\"../data/miPartitionColasce\")\n",
    "print(rdd_flat_coalesce.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Finding AVG\n",
    "\n",
    "Para esta practica vamos a trabajar con el dataset __movie_ratings.csv__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "movieRDD = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/movie_ratings.csv\")\n",
    "print(movieRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vamos a crear un nuevo RDD que tenga las __tuplas__ con (key, (val,1))donde key= pelicula y val= raiting y 1 es un numero constante que luego vamos a sumar para nos devuelva la __cantidad de ocurrencias__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Shawshank Redemption', (3, 1)), ('The Matrix', (5, 1)), ('12 Angry Men', (3, 1)), ('12 Angry Men', (4, 1)), ('The Matrix', (5, 1)), ('Pulp Fiction', (4, 1)), ('The Godfather', (5, 1)), ('The Shawshank Redemption', (2, 1)), ('Pulp Fiction', (4, 1)), ('The Godfather', (5, 1)), ('12 Angry Men', (2, 1)), ('The Godfather', (3, 1)), ('Pulp Fiction', (4, 1)), ('12 Angry Men', (1, 1)), ('The Shawshank Redemption', (2, 1)), ('12 Angry Men', (1, 1)), ('The Shawshank Redemption', (5, 1)), ('Pulp Fiction', (5, 1)), ('Pulp Fiction', (2, 1)), ('The Matrix', (4, 1))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "movieRDD_map = movieRDD.map(lambda x: (\\\n",
    "                                        x.split(\",\")[0], \\\n",
    "                                        (int(x.split(\",\")[1]),1)\n",
    "                                        )\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Luego reducimos con un __.reduceByKey__ para obtener algo así:\n",
    "(key, (suma(val), suma(1))) o lo que seria igual a \n",
    "(key, (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Shawshank Redemption', (12, 4)), ('12 Angry Men', (11, 5)), ('The Godfather', (13, 3)), ('The Matrix', (14, 3)), ('Pulp Fiction', (19, 5))]\n"
     ]
    }
   ],
   "source": [
    "movieRDD_reduce = movieRDD_map.reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "print(movieRDD_reduce.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que necesitamos hacer es una __lambda__ que reciba x y use x[0] para la __key__ x[1][0] para __la suma de los raitings__ y x[1][1] __para la cantidad de ocurrencias__ y calcule el promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Shawshank Redemption', 3.0),\n",
       " ('12 Angry Men', 2.2),\n",
       " ('The Godfather', 4.333333333333333),\n",
       " ('The Matrix', 4.666666666666667),\n",
       " ('Pulp Fiction', 3.8)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRDD_reduce.map(lambda x:(x[0], x[1][0]/x[1][1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Ejercicio rapido calcula AVG por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mesRDD = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/average_quiz_sample.csv\")\n",
    "print(mesRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este problema se resuelve como el ejemplo anterior de __AVG__ pero una vez que tenemos el dataset debemos eliminar la variable __city__ ya que solo nos importa __mes__ y __raiting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('JAN', 2.5), ('FEB', 1.25), ('MAR', 2.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesRDD_map = mesRDD.map(lambda x: x.split(\",\"))\n",
    "\n",
    "mesRDD_map2 = mesRDD_map.map(lambda x: (x[0], (int(float(x[2])), 1)))\n",
    "\n",
    "mesRDD_map2_reduce = mesRDD_map2.reduceByKey(lambda x, y: (x[0]+y[0], x[1] +y[1]) )\n",
    "\n",
    "mesRDD_map2_reduce.map(lambda x:(x[0], x[1][0] / x[1][1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Maximos y minimos en RDD\n",
    "\n",
    "Buscamos para cada pelicula el __maximo y minimo__ raiting para cada pelicula\n",
    "\n",
    "Para poder calcular Maximo y Minimo usamos __reduceByKey()__ donde esta transformacion toma los grupos de (key, val) y lo que queremos es que la __lambda x,y:  devuelva si x > y__\n",
    "\n",
    "Es importante destacar que esta comparación en una lambda devuelve _un booleano__ por __true o false__ así que hay que especificarle lo que queremos ver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creamos el __RDD__ con (Key, Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[('The Shawshank Redemption', 3), ('The Matrix', 5), ('12 Angry Men', 3), ('12 Angry Men', 4), ('The Matrix', 5), ('Pulp Fiction', 4), ('The Godfather', 5), ('The Shawshank Redemption', 2), ('Pulp Fiction', 4), ('The Godfather', 5), ('12 Angry Men', 2), ('The Godfather', 3), ('Pulp Fiction', 4), ('12 Angry Men', 1), ('The Shawshank Redemption', 2), ('12 Angry Men', 1), ('The Shawshank Redemption', 5), ('Pulp Fiction', 5), ('Pulp Fiction', 2), ('The Matrix', 4)]\n"
     ]
    }
   ],
   "source": [
    "movieRDD = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/movie_ratings.csv\")\n",
    "print(movieRDD.count())\n",
    "\n",
    "movieRDD_map = movieRDD.map(lambda x:(x.split(\",\")[0], int(x.split(\",\")[1]) ))\n",
    "print(movieRDD_map.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Usando __.reduceByKey()__ traemos el minimo por Pelicula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The Shawshank Redemption', 5),\n",
       " ('12 Angry Men', 4),\n",
       " ('The Godfather', 5),\n",
       " ('The Matrix', 5),\n",
       " ('Pulp Fiction', 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRDD_map.reduceByKey(lambda x, y: x  if x > y else y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Usando __.reduceByKey()__ traemos el maximo por Pelicula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The Shawshank Redemption', 2),\n",
       " ('12 Angry Men', 1),\n",
       " ('The Godfather', 3),\n",
       " ('The Matrix', 4),\n",
       " ('Pulp Fiction', 2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRDD_map.reduceByKey(lambda x, y: x  if x < y else y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Ejercicio rapido Maximos y Minimos.\n",
    "\n",
    "Usando el dataset __average_quiz_samples.csv__ mostrar para cada ciudad el maximo y minimo raiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cityRDD = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/average_quiz_sample.csv\")\n",
    "print(cityRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creamos el par (key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityRDD_map = cityRDD.map(lambda x: (x.split(\",\")[1], int(float(x.split(\",\")[2]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Buscamos el Maximo usando __lambda__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NY', 3), ('CT', 4), ('PA', 3), ('NJ', 2), ('VT', 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cityRDD_max = cityRDD_map.reduceByKey(lambda x,y: x if x > y else y)\n",
    "print(cityRDD_max.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Buscamos el Minimo usando __lambda__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NY', 1), ('CT', 4), ('PA', 1), ('NJ', 1), ('VT', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cityRDD_min = cityRDD_map.reduceByKey(lambda x,y: x if x < y else y)\n",
    "print(cityRDD_min.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Proyecto integrador\n",
    "\n",
    "Para este proyecto se pide usar el dataset __StudentData.csv__ y contestar las preguntas que estan en el __readme.md__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Students = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/StudentData.csv\")\n",
    "print(Students.count())\n",
    "#print(Students.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Elminamos el Header y nos quedamos solo con los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = Students.first()\n",
    "Students_one = Students.filter(lambda x:x != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. El numero de estudiantes en el File.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Students_one.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. El total de notas por estudiantes M y F.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Students_MF_RDD = Students_one.map(lambda x: \\\n",
    "    (\n",
    "        int(x.split(\",\")[0]),\n",
    "        x.split(',')[1],\n",
    "        x.split(',')[2],\n",
    "        x.split(',')[3],\n",
    "        x.split(',')[4],\n",
    "        int(x.split(',')[5]),\n",
    "        x.split(',')[6]\n",
    "    ))\n",
    "\n",
    "##Students_MF_RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Mostrar el total de alumnos que aprobaron o desaprobaron. Para aprobar se necesita 50+ puntos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Female', 29636), ('Male', 30461)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Students_MF_RDD.map(lambda x: (x[1],x[5])).reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Mostrar el total de alumnos que aprobaron o desaprobaron. Para aprobar se necesita 50+ puntos.\n",
    "\n",
    "\n",
    "Hay dos formas de obtener el numero de estudiantes que fallaron.\n",
    "1. Aplicando un filtro.\n",
    "2. Obtenemos uno de los dos y se lo restamos al total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 630\n"
     ]
    }
   ],
   "source": [
    "rdd_pass = Students_MF_RDD.filter(lambda x: x[5]> 50).count()\n",
    "rdd_fail = Students_MF_RDD.filter(lambda x: x[5] <= 50).count()\n",
    "print(rdd_fail, rdd_pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. El numero total de alumnos por curso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DB', 157),\n",
       " ('Cloud', 192),\n",
       " ('PF', 166),\n",
       " ('MVC', 157),\n",
       " ('OOP', 152),\n",
       " ('DSA', 176)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Students_MF_RDD.map(lambda x: (x[3], 1)).reduceByKey(lambda x,y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. El total de notas por curso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DB', 9270),\n",
       " ('Cloud', 11443),\n",
       " ('PF', 9933),\n",
       " ('MVC', 9585),\n",
       " ('OOP', 8916),\n",
       " ('DSA', 10950)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Students_MF_RDD.map(lambda x: (x[3], x[5])).reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. El AVG de notas de alumnos por curso.\n",
    "\n",
    "+ Usando __map__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DB', 59.044585987261144),\n",
       " ('Cloud', 59.598958333333336),\n",
       " ('PF', 59.83734939759036),\n",
       " ('MVC', 61.05095541401274),\n",
       " ('OOP', 58.6578947368421),\n",
       " ('DSA', 62.21590909090909)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Students_MF_RDD.map(lambda x: (x[3], (x[5], 1)))\\\n",
    "    .reduceByKey(lambda x,y : (x[0] + y[0], x[1] + y[1]))\\\n",
    "        .map(lambda x: (x[0], x[1][0]/ x[1][1]))\\\n",
    "            .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando __mapValue__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DB', 59.044585987261144),\n",
       " ('Cloud', 59.598958333333336),\n",
       " ('PF', 59.83734939759036),\n",
       " ('MVC', 61.05095541401274),\n",
       " ('OOP', 58.6578947368421),\n",
       " ('DSA', 62.21590909090909)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Students_MF_RDD.map(lambda x: (x[3], (x[5], 1)))\\\n",
    "    .reduceByKey(lambda x,y : (x[0] + y[0], x[1] + y[1]))\\\n",
    "        .mapValues(lambda x: (x[0] /  x[1]))\\\n",
    "            .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma es usando __mapValues()__ que por cada __key__ devuelve los valores, en este caso la __tupla__ haciendo que podamos trabajar directamente con ella."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. El maximo y minimo de notas por curso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DB', 98), ('Cloud', 99), ('PF', 99), ('MVC', 99), ('OOP', 99), ('DSA', 99)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Students_MF_RDD.map(lambda x: (x[3], x[5])).reduceByKey(lambda x,y: x if x > y else y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DB', 20), ('Cloud', 20), ('PF', 20), ('MVC', 22), ('OOP', 20), ('DSA', 20)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Students_MF_RDD.map(lambda x: (x[3], x[5])).reduceByKey(lambda x,y: x if x < y else y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. EL AVG de edad de M y F.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Female', 28.489021956087825), ('Male', 28.52304609218437)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Students_MF_RDD.map(lambda x: (x[1], (x[0], 1)))\\\n",
    "    .reduceByKey(lambda x,y:(x[0]+y[0], x[1]+y[1]))\\\n",
    "        .mapValues(lambda x: x[0] / x[1] ).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver este ultimo problema usamos __mapValue__ que lo aprendimos en el ejemplo anterior pero tambien podriamos usar __map__ y usar los indices y subindices de la __tupla__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
