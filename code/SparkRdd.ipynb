{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creamos un contexto nuevo de Spark para leer un archivo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/27 20:50:23 WARN Utils: Your hostname, DESKTOP-SLEQT56 resolves to a loopback address: 127.0.1.1; using 172.25.13.138 instead (on interface eth0)\n",
      "24/02/27 20:50:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/27 20:50:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('readFile')\n",
    "spark = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/sampletext.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 2 23 4', '10 23 101 67', '45 65 34 56', '101 54 34 101']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sobre el __rdd__ aplicamos una __transformacion__ map que devuelve otro __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "miFile_rdd = text.map(lambda x:x.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(miFile_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2', '23', '4'],\n",
       " ['10', '23', '101', '67'],\n",
       " ['45', '65', '34', '56'],\n",
       " ['101', '54', '34', '101']]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miFile_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. En este ejemplo queremos usar una funcion __foo__ definida por el usuario donde, primero hagamos lo mismo que se hizo en __2__ pero luego se castea cada elemento a __int__ y se le suma 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 25, 6], [12, 25, 103, 69], [47, 67, 36, 58], [103, 56, 36, 103]]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mi_foo(x):\n",
    "    l =  x.split(' ')\n",
    "    ls = []\n",
    "    for num in l:\n",
    "        ls.append(int(num) + 2)\n",
    "    return ls\n",
    "\n",
    "miFile  = text.map(mi_foo)\n",
    "miFile.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio.\n",
    "\n",
    "Tenemos un archivo con saludos (texto) y debemos devolver un array con la logitud de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileText = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/wordcount.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi how are you?', 'Hope you are doing', 'great']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 3, 4], [4, 3, 3, 5], [5]]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(x):\n",
    "    array = x.split(' ')\n",
    "    l2 = []\n",
    "    for word in array:\n",
    "        l2.append(len(word))\n",
    "    return l2\n",
    "\n",
    "fileText.map(foo).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variacion del ejercicio.\n",
    "\n",
    "En lugar de usar una __UDF__ usamos una funcion __lambda__ con una __list comprehention__ para obtener el mismo resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 3, 4], [4, 3, 3, 5], [5]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText.map(lambda x: [len(word) for word in x.split(' ')]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. FlatMap\n",
    "\n",
    "Devuelve todos los elementos en un tipo de dato __unificado__  generando un nuevo __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'how', 'are', 'you?', 'Hope', 'you', 'are', 'doing', 'great']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText.flatMap(lambda x:x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. filter - lambda\n",
    "\n",
    "Se usa para segmentar el resultado de una __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.filter(lambda x: x in ('10 23 45 67', '87 54 34 101')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar filter sin __lambda function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(x):\n",
    "    if x in ('10 23 45 67', '87 54 34 101'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "text.filter(foo).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 Ejercicio 2\n",
    "\n",
    "Construir un filtro que elimine las palabras que comiencen con __a__ o __c__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this mango company animal',\n",
       " 'cat dog ant mic laptop',\n",
       " 'chair seitch mobile am charger cover',\n",
       " 'amanda any alarm ant',\n",
       " 'este es mi ejemplo']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/quiz2.txt\")\n",
    "fileText.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company', 'animal', 'cat', 'ant', 'chair', 'am', 'charger', 'cover', 'amanda', 'any', 'alarm', 'ant']\n"
     ]
    }
   ],
   "source": [
    "rdd_rts = fileText.flatMap(lambda x:x.split(' '))\\\n",
    "    .filter(lambda x: x.startswith(\"a\") or x.startswith(\"c\"))\n",
    "print(rdd_rts.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'mango', 'dog', 'mic', 'laptop', 'seitch', 'mobile', 'este', 'es', 'mi', 'ejemplo']\n"
     ]
    }
   ],
   "source": [
    "def foo(x):\n",
    "    if x.startswith(\"a\") or x.startswith(\"c\"):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "rdd_rts = fileText.flatMap(lambda x:x.split(' ')).filter(foo)\n",
    "print(rdd_rts.collect())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. RDD Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '4', '10', '45', '34', '56', '54', '2', '23', '101', '67', '65']\n"
     ]
    }
   ],
   "source": [
    "rdd_text = text.flatMap(lambda x:x.split(' ')).distinct()\n",
    "print(rdd_text.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2', '23', '4'],\n",
       " ['10', '23', '101', '67'],\n",
       " ['45', '65', '34', '56'],\n",
       " ['101', '54', '34', '101']]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.map(lambda x: x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. GroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', [1]), ('4', [1]), ('10', [1]), ('45', [1]), ('34', [1, 1]), ('56', [1]), ('54', [1]), ('2', [1]), ('23', [1, 1]), ('101', [1, 1, 1]), ('67', [1]), ('65', [1])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_split = text.flatMap(lambda x: x.split(' '))\n",
    "rdd_key_val = rdd_split.map(lambda x: (x, 1))\n",
    "rdd_reduce = rdd_key_val.groupByKey()\n",
    "rdd_mapVal = rdd_reduce.mapValues(list).collect()\n",
    "print(rdd_mapVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. reduceByKey\n",
    "\n",
    "Similar al GroupByKey pero devuelve un __rdd__ agrupado con una __operacion__ sobre los valores de las __keys__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 1), ('4', 1), ('10', 1), ('45', 1), ('34', 2), ('56', 1), ('54', 1), ('2', 1), ('23', 2), ('101', 3), ('67', 1), ('65', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_split = text.flatMap(lambda x:x.split(' '))\n",
    "rdd_map = rdd_split.map(lambda x: (x, 1))\n",
    "rdd_reduce = rdd_map.reduceByKey(lambda x,y: x + y)\n",
    "print(rdd_reduce.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Ejercicio \n",
    "\n",
    "Escribir un script que devuelva la cantidad de cada palabra contenida en el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 1), ('mango', 1), ('cat', 1), ('ant', 2), ('laptop', 1), ('chair', 1), ('mobile', 1), ('am', 1), ('este', 1), ('mi', 1), ('ejemplo', 1), ('company', 1), ('animal', 1), ('dog', 1), ('mic', 1), ('seitch', 1), ('charger', 1), ('cover', 1), ('amanda', 1), ('any', 1), ('alarm', 1), ('es', 1)]\n"
     ]
    }
   ],
   "source": [
    "textFile = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/quiz2.txt\")\n",
    "type(textFile)\n",
    "\n",
    "#textFile.collect()\n",
    "rdd_flat= textFile.flatMap(lambda x:x.split(' '))\n",
    "rdd_key_val = rdd_flat.map(lambda x: (x, 1))\n",
    "rdd_reduce = rdd_key_val.reduceByKey(lambda x, y: x + y)\n",
    "print(rdd_reduce.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Count y CountByValue\n",
    "\n",
    "Ambas son acciones que se aplican sobre __RDD__\n",
    "Count devuelve la cantidad de elementos dentro de un __rdd__\n",
    "CountByValue devuelve la cantidad de veces que una elemento aparece en un __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "23\n",
      "defaultdict(<class 'int'>, {'this': 1, 'mango': 1, 'company': 1, 'animal': 1, 'cat': 1, 'dog': 1, 'ant': 2, 'mic': 1, 'laptop': 1, 'chair': 1, 'seitch': 1, 'mobile': 1, 'am': 1, 'charger': 1, 'cover': 1, 'amanda': 1, 'any': 1, 'alarm': 1, 'este': 1, 'es': 1, 'mi': 1, 'ejemplo': 1})\n"
     ]
    }
   ],
   "source": [
    "print(textFile.count())\n",
    "print(rdd_key_val.count())\n",
    "print(rdd_flat.countByValue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ant : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for key, value in rdd_flat.countByValue().items():\n",
    "    if value > 1:\n",
    "        print (key, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Rdd saveAsTextFile\n",
    "\n",
    "Se usa para guardar un __rdd__ como un archivo de texto. POr defecto un __rdd__ está compuesto por dos particiones, asique __vamos a ver dos archivos__ con el 50% de los datos cada uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(rdd_flat.getNumPartitions())\n",
    "rdd_flat.saveAsTextFile(\"../data/miRDDTextFile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Aumentar numero de particiones\n",
    "\n",
    "Se puede usar __repartition__ para aumentar o __coalesce__ para disminuirlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "MapPartitionsRDD[242] at coalesce at NativeMethodAccessorImpl.java:0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "textFile = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/quiz2.txt\")\n",
    "\n",
    "\n",
    "rdd_flat = textFile.flatMap(lambda x:x.split(' '))\n",
    "print(rdd_flat.getNumPartitions())\n",
    "\n",
    "rdd_flat_4 = rdd_flat.repartition(4)\n",
    "rdd_flat_4.saveAsTextFile(\"../data/miPartition\")\n",
    "print(rdd_flat_4.getNumPartitions())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
