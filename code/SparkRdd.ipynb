{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creamos un contexto nuevo de Spark para leer un archivo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/28 20:44:40 WARN Utils: Your hostname, DESKTOP-SLEQT56 resolves to a loopback address: 127.0.1.1; using 172.25.13.138 instead (on interface eth0)\n",
      "24/02/28 20:44:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/28 20:44:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/28 20:44:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/02/28 20:44:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('readFile')\n",
    "spark = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/sampletext.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 2 23 4', '10 23 101 67', '45 65 34 56', '101 54 34 101']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sobre el __rdd__ aplicamos una __transformacion__ map que devuelve otro __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "miFile_rdd = text.map(lambda x:x.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(miFile_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2', '23', '4'],\n",
       " ['10', '23', '101', '67'],\n",
       " ['45', '65', '34', '56'],\n",
       " ['101', '54', '34', '101']]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miFile_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. En este ejemplo queremos usar una funcion __foo__ definida por el usuario donde, primero hagamos lo mismo que se hizo en __2__ pero luego se castea cada elemento a __int__ y se le suma 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 25, 6], [12, 25, 103, 69], [47, 67, 36, 58], [103, 56, 36, 103]]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mi_foo(x):\n",
    "    l =  x.split(' ')\n",
    "    ls = []\n",
    "    for num in l:\n",
    "        ls.append(int(num) + 2)\n",
    "    return ls\n",
    "\n",
    "miFile  = text.map(mi_foo)\n",
    "miFile.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio.\n",
    "\n",
    "Tenemos un archivo con saludos (texto) y debemos devolver un array con la logitud de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileText = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/wordcount.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi how are you?', 'Hope you are doing', 'great']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 3, 4], [4, 3, 3, 5], [5]]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(x):\n",
    "    array = x.split(' ')\n",
    "    l2 = []\n",
    "    for word in array:\n",
    "        l2.append(len(word))\n",
    "    return l2\n",
    "\n",
    "fileText.map(foo).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variacion del ejercicio.\n",
    "\n",
    "En lugar de usar una __UDF__ usamos una funcion __lambda__ con una __list comprehention__ para obtener el mismo resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 3, 4], [4, 3, 3, 5], [5]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText.map(lambda x: [len(word) for word in x.split(' ')]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. FlatMap\n",
    "\n",
    "Devuelve todos los elementos en un tipo de dato __unificado__  generando un nuevo __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'how', 'are', 'you?', 'Hope', 'you', 'are', 'doing', 'great']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText.flatMap(lambda x:x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. filter - lambda\n",
    "\n",
    "Se usa para segmentar el resultado de una __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.filter(lambda x: x in ('10 23 45 67', '87 54 34 101')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar filter sin __lambda function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(x):\n",
    "    if x in ('10 23 45 67', '87 54 34 101'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "text.filter(foo).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 Ejercicio 2\n",
    "\n",
    "Construir un filtro que elimine las palabras que comiencen con __a__ o __c__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this mango company animal',\n",
       " 'cat dog ant mic laptop',\n",
       " 'chair seitch mobile am charger cover',\n",
       " 'amanda any alarm ant',\n",
       " 'este es mi ejemplo']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileText = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/quiz2.txt\")\n",
    "fileText.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company', 'animal', 'cat', 'ant', 'chair', 'am', 'charger', 'cover', 'amanda', 'any', 'alarm', 'ant']\n"
     ]
    }
   ],
   "source": [
    "rdd_rts = fileText.flatMap(lambda x:x.split(' '))\\\n",
    "    .filter(lambda x: x.startswith(\"a\") or x.startswith(\"c\"))\n",
    "print(rdd_rts.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'mango', 'dog', 'mic', 'laptop', 'seitch', 'mobile', 'este', 'es', 'mi', 'ejemplo']\n"
     ]
    }
   ],
   "source": [
    "def foo(x):\n",
    "    if x.startswith(\"a\") or x.startswith(\"c\"):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "rdd_rts = fileText.flatMap(lambda x:x.split(' ')).filter(foo)\n",
    "print(rdd_rts.collect())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. RDD Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '4', '10', '45', '34', '56', '54', '2', '23', '101', '67', '65']\n"
     ]
    }
   ],
   "source": [
    "rdd_text = text.flatMap(lambda x:x.split(' ')).distinct()\n",
    "print(rdd_text.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2', '23', '4'],\n",
       " ['10', '23', '101', '67'],\n",
       " ['45', '65', '34', '56'],\n",
       " ['101', '54', '34', '101']]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.map(lambda x: x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. GroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', [1]), ('4', [1]), ('10', [1]), ('45', [1]), ('34', [1, 1]), ('56', [1]), ('54', [1]), ('2', [1]), ('23', [1, 1]), ('101', [1, 1, 1]), ('67', [1]), ('65', [1])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_split = text.flatMap(lambda x: x.split(' '))\n",
    "rdd_key_val = rdd_split.map(lambda x: (x, 1))\n",
    "rdd_reduce = rdd_key_val.groupByKey()\n",
    "rdd_mapVal = rdd_reduce.mapValues(list).collect()\n",
    "print(rdd_mapVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. reduceByKey\n",
    "\n",
    "Similar al GroupByKey pero devuelve un __rdd__ agrupado con una __operacion__ sobre los valores de las __keys__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 1), ('4', 1), ('10', 1), ('45', 1), ('34', 2), ('56', 1), ('54', 1), ('2', 1), ('23', 2), ('101', 3), ('67', 1), ('65', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_split = text.flatMap(lambda x:x.split(' '))\n",
    "rdd_map = rdd_split.map(lambda x: (x, 1))\n",
    "rdd_reduce = rdd_map.reduceByKey(lambda x,y: x + y)\n",
    "print(rdd_reduce.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Ejercicio \n",
    "\n",
    "Escribir un script que devuelva la cantidad de cada palabra contenida en el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 1), ('mango', 1), ('cat', 1), ('ant', 2), ('laptop', 1), ('chair', 1), ('mobile', 1), ('am', 1), ('este', 1), ('mi', 1), ('ejemplo', 1), ('company', 1), ('animal', 1), ('dog', 1), ('mic', 1), ('seitch', 1), ('charger', 1), ('cover', 1), ('amanda', 1), ('any', 1), ('alarm', 1), ('es', 1)]\n"
     ]
    }
   ],
   "source": [
    "textFile = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/quiz2.txt\")\n",
    "type(textFile)\n",
    "\n",
    "#textFile.collect()\n",
    "rdd_flat= textFile.flatMap(lambda x:x.split(' '))\n",
    "rdd_key_val = rdd_flat.map(lambda x: (x, 1))\n",
    "rdd_reduce = rdd_key_val.reduceByKey(lambda x, y: x + y)\n",
    "print(rdd_reduce.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Count y CountByValue\n",
    "\n",
    "Ambas son acciones que se aplican sobre __RDD__\n",
    "Count devuelve la cantidad de elementos dentro de un __rdd__\n",
    "CountByValue devuelve la cantidad de veces que una elemento aparece en un __rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "23\n",
      "defaultdict(<class 'int'>, {'this': 1, 'mango': 1, 'company': 1, 'animal': 1, 'cat': 1, 'dog': 1, 'ant': 2, 'mic': 1, 'laptop': 1, 'chair': 1, 'seitch': 1, 'mobile': 1, 'am': 1, 'charger': 1, 'cover': 1, 'amanda': 1, 'any': 1, 'alarm': 1, 'este': 1, 'es': 1, 'mi': 1, 'ejemplo': 1})\n"
     ]
    }
   ],
   "source": [
    "print(textFile.count())\n",
    "print(rdd_key_val.count())\n",
    "print(rdd_flat.countByValue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ant : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for key, value in rdd_flat.countByValue().items():\n",
    "    if value > 1:\n",
    "        print (key, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Rdd saveAsTextFile\n",
    "\n",
    "Se usa para guardar un __rdd__ como un archivo de texto. POr defecto un __rdd__ está compuesto por dos particiones, asique __vamos a ver dos archivos__ con el 50% de los datos cada uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(rdd_flat.getNumPartitions())\n",
    "rdd_flat.saveAsTextFile(\"../data/miRDDTextFile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Aumentar numero de particiones\n",
    "\n",
    "Se puede usar __repartition__ para aumentar o __coalesce__ para disminuirlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "textFile = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/quiz2.txt\")\n",
    "\n",
    "\n",
    "rdd_flat = textFile.flatMap(lambda x:x.split(' '))\n",
    "print(rdd_flat.getNumPartitions())\n",
    "\n",
    "rdd_flat_4 = rdd_flat.repartition(4)\n",
    "rdd_flat_4.saveAsTextFile(\"../data/miPartition\")\n",
    "print(rdd_flat_4.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EN el ejemplo anterior vemos que se generan 4 archivos de particiones, ahora podemos usar __coalesce__ para que nos quede solo una particion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(rdd_flat_4.getNumPartitions())\n",
    "rdd_flat_coalesce = rdd_flat_4.coalesce(1)\n",
    "rdd_flat_coalesce.saveAsTextFile(\"../data/miPartitionColasce\")\n",
    "print(rdd_flat_coalesce.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Finding AVG\n",
    "\n",
    "Para esta practica vamos a trabajar con el dataset __movie_ratings.csv__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "movieRDD = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/movie_ratings.csv\")\n",
    "print(movieRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vamos a crear un nuevo RDD que tenga las __tuplas__ con (key, (val,1))donde key= pelicula y val= raiting y 1 es un numero constante que luego vamos a sumar para nos devuelva la __cantidad de ocurrencias__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Shawshank Redemption', (3, 1)), ('The Matrix', (5, 1)), ('12 Angry Men', (3, 1)), ('12 Angry Men', (4, 1)), ('The Matrix', (5, 1)), ('Pulp Fiction', (4, 1)), ('The Godfather', (5, 1)), ('The Shawshank Redemption', (2, 1)), ('Pulp Fiction', (4, 1)), ('The Godfather', (5, 1)), ('12 Angry Men', (2, 1)), ('The Godfather', (3, 1)), ('Pulp Fiction', (4, 1)), ('12 Angry Men', (1, 1)), ('The Shawshank Redemption', (2, 1)), ('12 Angry Men', (1, 1)), ('The Shawshank Redemption', (5, 1)), ('Pulp Fiction', (5, 1)), ('Pulp Fiction', (2, 1)), ('The Matrix', (4, 1))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "movieRDD_map = movieRDD.map(lambda x: (\\\n",
    "                                        x.split(\",\")[0], \\\n",
    "                                        (int(x.split(\",\")[1]),1)\n",
    "                                        )\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Luego reducimos con un __.reduceByKey__ para obtener algo así:\n",
    "(key, (suma(val), suma(1))) o lo que seria igual a \n",
    "(key, (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Shawshank Redemption', (12, 4)), ('12 Angry Men', (11, 5)), ('The Godfather', (13, 3)), ('The Matrix', (14, 3)), ('Pulp Fiction', (19, 5))]\n"
     ]
    }
   ],
   "source": [
    "movieRDD_reduce = movieRDD_map.reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "print(movieRDD_reduce.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que necesitamos hacer es una __lambda__ que reciba x y use x[0] para la __key__ x[1][0] para __la suma de los raitings__ y x[1][1] __para la cantidad de ocurrencias__ y calcule el promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Shawshank Redemption', 3.0),\n",
       " ('12 Angry Men', 2.2),\n",
       " ('The Godfather', 4.333333333333333),\n",
       " ('The Matrix', 4.666666666666667),\n",
       " ('Pulp Fiction', 3.8)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRDD_reduce.map(lambda x:(x[0], x[1][0]/x[1][1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Ejercicio rapido calcula AVG por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "mesRDD = spark.textFile(\"/mnt/d/Proyectos/Tutorial-SparkAWS/data/average_quiz_sample.csv\")\n",
    "print(mesRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JAN', (10, 4)), ('FEB', (5, 4)), ('MAR', (8, 4))]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesRDD_map = mesRDD.map(lambda x: x.split(\",\"))\n",
    "\n",
    "mesRDD_map2 = mesRDD_map.map(lambda x: (x[0], (int(float(x[2])), 1)))\n",
    "\n",
    "mesRDD_map2_reduce = mesRDD_map2.reduceByKey(lambda x, y: (x[0]+y[0], x[1] +y[1]) )\n",
    "\n",
    "\n",
    "\n",
    "mesRDD_map2_reduce.map(lambda x:(x[0], x[1][0] / x[1][1])).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
